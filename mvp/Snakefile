
from os.path import join
from os import listdir
import os
import re
import yaml
import boto3

configfile: "./config.yaml"

RESOURCESYAML=config['resources']

with open(RESOURCESYAML) as json_file:
    CLUSTER = yaml.safe_load(json_file)
getthreads=lambda rname:int(CLUSTER[rname]["threads"]) if rname in CLUSTER and "threads" in CLUSTER[rname] else int(CLUSTER["__default__"]["threads"])
getmemg=lambda rname:CLUSTER[rname]["mem"] if rname in CLUSTER and "mem" in CLUSTER[rname] else CLUSTER["__default__"]["mem"]
getmemG=lambda rname:getmemg(rname).replace("g","G")

from snakemake.remote.S3 import RemoteProvider as S3RemoteProvider
S3 = S3RemoteProvider()

s3 = boto3.resource('s3')
my_bucket = s3.Bucket('ccr-genomics-testdata')

for object_summary in my_bucket.objects.filter(Prefix="chr"):
    print(object_summary.key)


SAMPLES = ["Test1_R_T"]

#workpath="./"


rule all:
	input:
#		expand(join("{sample}","fastqc","{sample}_R1_fastqc.zip"),sample=SAMPLES),
#		expand(join("{sample}","multiqc","multiqc_report.html"),sample=SAMPLES),
		expand(join("{sample}","star_out","{sample}.star.bam"),sample=SAMPLES),
		expand(join("{sample}","star_out","{sample}.star_transcriptome.bam"),sample=SAMPLES),


rule fastqc:
	input:
		R1 = S3.remote("ccr-genomics-testdata/testdata/{sample}_R1.fastq.gz", keep_local=True),
		R2 = S3.remote("ccr-genomics-testdata/testdata/{sample}_R2.fastq.gz", keep_local=True),
	params:
		out = join("{sample}","fastqc")
	output:
		join("{sample}","fastqc","{sample}_R1_fastqc.zip"),
		join("{sample}","fastqc","{sample}_R2_fastqc.zip")

	conda: "env.yaml"

	threads: getthreads("fastqc")

#	container: "docker://nciccbr/ccbr_fastqc_0.11.9:v1.1"

	shell: """

	fastqc {input} -o  {params.out}

	"""

rule multiqc:
	input:
		join("{sample}","fastqc","{sample}_R1_fastqc.zip"),
		join("{sample}","fastqc","{sample}_R2_fastqc.zip")
	params:
		out = join("{sample}","multiqc")
	output:
		join("{sample}","multiqc","multiqc_report.html")

	conda: "env.yaml"

	threads: getthreads("fastqc")

	shell: """

	mkdir -p {params.out}
	multiqc {input} -o {params.out}

	"""

rule star:
	input:
		R1 = S3.remote("ccr-genomics-testdata/testdata/{sample}_R1.fastq.gz", keep_local=True),
		R2 = S3.remote("ccr-genomics-testdata/testdata/{sample}_R2.fastq.gz", keep_local=True),
		STARgenome = S3.remote("ccr-genomics-testdata/References/index-STAR_2.7.9a.tar", keep_local=True),
		gtf = S3.remote("ccr-genomics-testdata/References/gencode.v37lift37.annotation_ERCC92.gtf", keep_local=True),
	params:
		out = join("{sample}","star_out"),
#		STARgenome = S3.remote("ccr-genomics-testdata/References/index-STAR_2.7.9a.tar", keep_local=True),
#		gtf = S3.remote("ccr-genomics-testdata/References/gencode.v37lift37.annotation_ERCC92.gtf", keep_local=True),
	output:
		G_bam = join("{sample}","star_out","{sample}.star.bam"),
		T_bam = join("{sample}","star_out","{sample}.star_transcriptome.bam")

	threads: getthreads("star")

	conda: "env.yaml"

	shell: """
#	set +u; source /usr/local/bin/virtualenv; set -u
#	cd {params.out}
	
	indexdir=$(dirname {input.STARgenome})
	indexdir="${{indexdir}}/index-STAR_2.7.9a"
	
	if  [ ! -f ${{indexdir}}/SA ];
	then
	tar -xvf {input.STARgenome} 
	fi

	STAR --genomeDir $indexdir \
		--readFilesIn  {input.R1} {input.R2} \
		--sjdbGTFfile {input.gtf} \
		--readFilesCommand zcat \
		--runThreadN {threads} \
		--twopassMode Basic \
		--outSAMunmapped Within \
		--chimSegmentMin 12 \
		--chimJunctionOverhangMin 12 \
		--alignSJDBoverhangMin 10 \
		--alignMatesGapMax 100000 \
		--chimSegmentReadGapMax 3 \
		--outFilterMismatchNmax 2 \
		--outSAMtype BAM SortedByCoordinate \
		--quantMode TranscriptomeSAM \
		--outBAMsortingThreadN 6 \
		--limitBAMsortRAM 80000000000
	mv *Aligned.sortedByCoord.out.bam {params.out}/{output.G_bam}
	mv *Aligned.toTranscriptome.out.bam {params.out}/{output.T_bam}
	"""

